"""RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏."""

import os
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from domain.interfaces.rag import RAGInterface
from infrastructure.logging.rich_logger import RichLogger

logger = logging.getLogger(__name__)


class RAGSystem(RAGInterface):
    """RAG —Å–∏—Å—Ç–µ–º–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ChromaDB –∏ Sentence Transformers."""

    def __init__(
        self,
        embedding_model: str = "all-MiniLM-L6-v2",
        vector_db_path: str = "./data/vector_db",
        index_path: str = "./data/index",
        ollama_llm=None,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã.

        Args:
            embedding_model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            vector_db_path: –ü—É—Ç—å –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
            index_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞
            ollama_llm: –≠–∫–∑–µ–º–ø–ª—è—Ä Ollama LLM (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)
        """
        self.embedding_model_name = embedding_model
        self.vector_db_path = vector_db_path
        self.index_path = index_path
        self.ollama_llm = ollama_llm

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        logger.info(f"Loading embedding model: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB
        os.makedirs(vector_db_path, exist_ok=True)
        os.makedirs(index_path, exist_ok=True)

        self.client = chromadb.PersistentClient(
            path=vector_db_path,
            settings=Settings(anonymized_telemetry=False)
        )

        self.collection = self.client.get_or_create_collection(
            name="documents",
            metadata={"hnsw:space": "cosine"}
        )

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        self.metadata_index = self._load_metadata_index()

    def _load_metadata_index(self) -> Dict[str, Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö."""
        index_file = os.path.join(self.index_path, "metadata.json")
        if os.path.exists(index_file):
            with open(index_file, "r", encoding="utf-8") as f:
                return json.load(f)
        return {}

    def _save_metadata_index(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö."""
        index_file = os.path.join(self.index_path, "metadata.json")
        with open(index_file, "w", encoding="utf-8") as f:
            json.dump(self.metadata_index, f, ensure_ascii=False, indent=2)

    async def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤.

        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        if self.ollama_llm:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Ollama –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            return await self.ollama_llm.generate_embeddings(texts)
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Sentence Transformers (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤)
            embeddings = self.embedding_model.encode(texts, show_progress_bar=False)
            return embeddings.tolist()

    def _split_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """
        –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏.

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
            overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏

        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
        """
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0

        for word in words:
            word_length = len(word) + 1  # +1 –¥–ª—è –ø—Ä–æ–±–µ–ª–∞
            if current_length + word_length > chunk_size and current_chunk:
                chunks.append(" ".join(current_chunk))
                # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
                overlap_words = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk
                current_chunk = overlap_words + [word]
                current_length = sum(len(w) + 1 for w in current_chunk)
            else:
                current_chunk.append(word)
                current_length += word_length

        if current_chunk:
            chunks.append(" ".join(current_chunk))

        return chunks

    async def index_documents(self, documents: List[Dict[str, Any]]) -> None:
        """
        –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

        Args:
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏: content, filepath, metadata
        """
        all_chunks = []
        all_embeddings = []
        all_ids = []
        all_metadatas = []

        for doc_idx, doc in enumerate(documents):
            content = doc.get("content", "")
            filepath = doc.get("filepath", f"doc_{doc_idx}")
            metadata = doc.get("metadata", {})

            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self._split_text(content)

            for chunk_idx, chunk in enumerate(chunks):
                chunk_id = f"{filepath}_{chunk_idx}"
                all_chunks.append(chunk)
                all_ids.append(chunk_id)
                all_metadatas.append({
                    "filepath": filepath,
                    "chunk_index": chunk_idx,
                    **metadata,
                })

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        logger.info(f"üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(all_chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")
        all_embeddings = await self._generate_embeddings(all_chunks)
        logger.info("‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ ChromaDB
        self.collection.add(
            embeddings=all_embeddings,
            documents=all_chunks,
            ids=all_ids,
            metadatas=all_metadatas,
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        for doc in documents:
            filepath = doc.get("filepath", "")
            self.metadata_index[filepath] = {
                "filepath": filepath,
                "metadata": doc.get("metadata", {}),
            }

        self._save_metadata_index()
        logger.info(f"‚úÖ –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({len(all_chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤)")

    async def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º.

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        logger.info(f"üîç –í—ã–ø–æ–ª–Ω—è—é RAG –ø–æ–∏—Å–∫: '{query}' (top_k={top_k})")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embeddings = await self._generate_embeddings([query])
        query_embedding = query_embeddings[0]

        # –ü–æ–∏—Å–∫ –≤ ChromaDB
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
        )

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        documents = []
        if results["documents"] and len(results["documents"][0]) > 0:
            for i in range(len(results["documents"][0])):
                documents.append({
                    "content": results["documents"][0][i],
                    "filepath": results["metadatas"][0][i].get("filepath", ""),
                    "metadata": results["metadatas"][0][i],
                    "distance": results["distances"][0][i] if results.get("distances") else None,
                })
            
            # –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            RichLogger.log_rag_search(query, documents, top_k)
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(documents)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        else:
            logger.warning("‚ö†Ô∏è  RAG –ø–æ–∏—Å–∫ –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

        return documents

    async def get_context(self, query: str, top_k: int = 5) -> str:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞.

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Returns:
            –°—Ç—Ä–æ–∫–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        """
        documents = await self.search(query, top_k)
        if not documents:
            return ""

        context_parts = []
        for doc in documents:
            filepath = doc.get("filepath", "unknown")
            content = doc.get("content", "")
            context_parts.append(f"–§–∞–π–ª: {filepath}\n{content}\n")

        return "\n---\n".join(context_parts)


class DocumentIndexer:
    """–ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ .md —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞."""

    def __init__(self, project_root: str = "."):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞.

        Args:
            project_root: –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞
        """
        self.project_root = Path(project_root)

    def find_markdown_files(self) -> List[Path]:
        """
        –ü–æ–∏—Å–∫ –≤—Å–µ—Ö .md —Ñ–∞–π–ª–æ–≤ –≤ –ø—Ä–æ–µ–∫—Ç–µ.

        Returns:
            –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ .md —Ñ–∞–π–ª–∞–º
        """
        md_files = []
        for md_file in self.project_root.rglob("*.md"):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã –≤ —Å–ª—É–∂–µ–±–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö
            if any(part.startswith(".") for part in md_file.parts):
                continue
            md_files.append(md_file)
        return md_files

    def index_project(self) -> List[Dict[str, Any]]:
        """
        –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö .md —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞.

        Returns:
            –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        """
        md_files = self.find_markdown_files()
        logger.info(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(md_files)} .md —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        documents = []

        for idx, md_file in enumerate(md_files, 1):
            try:
                with open(md_file, "r", encoding="utf-8") as f:
                    content = f.read()

                rel_path = md_file.relative_to(self.project_root)
                RichLogger.log_indexing_progress(idx, len(md_files), str(rel_path))
                
                documents.append({
                    "content": content,
                    "filepath": str(rel_path),
                    "metadata": {
                        "type": "markdown",
                        "size": len(content),
                    },
                })
            except Exception as e:
                logger.error(f"Error reading {md_file}: {e}")

        return documents
